<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Machine_learning_1</title>
      <link href="/2024/08/04/Machine-learning-1/"/>
      <url>/2024/08/04/Machine-learning-1/</url>
      
        <content type="html"><![CDATA[<h1 id="李宏毅机器学习笔记（一）"><a href="#李宏毅机器学习笔记（一）" class="headerlink" title="李宏毅机器学习笔记（一）"></a>李宏毅机器学习笔记（一）</h1><span id="more"></span><h1 id="问题种类：regression-、-classification-、-structuredLearning"><a href="#问题种类：regression-、-classification-、-structuredLearning" class="headerlink" title="问题种类：regression 、 classification 、 structuredLearning"></a>问题种类：regression 、 classification 、 structuredLearning</h1><p>$$<br>y &#x3D; w * x + b (linear)<br>$$</p><ul><li>训练合适的w和b</li></ul><h2 id="定义loss函数："><a href="#定义loss函数：" class="headerlink" title="定义loss函数："></a>定义loss函数：</h2><ul><li>MAE： regression中预测值与label之间差的绝对值和</li><li>MSE： regression中预测值与label之间差的平方和</li><li>Cross-Entropy：classification中预测值与label的交叉熵</li></ul><h2 id="training：找到loss最小的feature"><a href="#training：找到loss最小的feature" class="headerlink" title="training：找到loss最小的feature"></a>training：找到loss最小的feature</h2><h2 id="optimize"><a href="#optimize" class="headerlink" title="optimize"></a>optimize</h2><ul><li>通常使用梯度下降法：对各个feature求偏导得到梯度，向梯度减小的方向修改参数</li><li>tips：对于斜率变化小的case，在梯度下降的过程中使用相同的步长将很难达到最优的loss。通常使用adam(pytorch自带)、冲量梯度下降法来优化</li></ul><h1 id="DNN"><a href="#DNN" class="headerlink" title="DNN"></a>DNN</h1><p>上述linear的方程没法预测更多一般的情况，通常采用多个ReLU函数或者Sigmoid函数来逼近真实的分布</p><h3 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h3><p>$$y &#x3D; b + \sum_i c_i sigmoid(y_j)$$<br>上式$y_j$也可以写成$y_j &#x3D; b + \sum_j w_jx_j $<br>有$$y &#x3D; b + \sum_i c_i sigmoid(b_i + \sum_j w_{ij}x_i)$$<br>写成矩阵形式<br>$$<br> y &#x3D; b + \begin{bmatrix} c\end{bmatrix}^T + sigmoid(b + \begin{bmatrix} W\end{bmatrix}\begin{bmatrix} x\end{bmatrix})<br>$$</p><ul><li>上式的$y$当作下一层的输入，又可以套一层神经网路，即是简单的DNN模型</li></ul><h3 id="批处理"><a href="#批处理" class="headerlink" title="批处理"></a>批处理</h3><p>训练时候将数据分成多个batch，每个batch计算出一个loss，根据loss来计算梯度，更新feature。</p><ul><li>全部batch计算完称为一个epoch</li></ul><h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><p>CNN通常用于图像操作，输入是一幅图像</p><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p>对于输入图像看作一个矩阵$\begin{bmatrix} Image\end{bmatrix}$ , 定义一个卷积核矩阵$\begin{bmatrix} Filter\end{bmatrix}$与其上的数字相乘相加得到一个值称为一次卷积<br>通过训练出一系列的卷积核一层一层的对图像进行卷积运算提取相关的特征</p><h3 id="一些问题及解决"><a href="#一些问题及解决" class="headerlink" title="一些问题及解决"></a>一些问题及解决</h3><ul><li>卷积没法超出边界导致卷积结果大小 &lt; 原图大小 &#x3D;&gt; 通过给图像边界补0（padding）</li><li>每次卷积都对整个图像操作，计算量大，参数多，容易overfitting &#x3D;&gt; 设定Receptive Field</li></ul><h4 id="Receptive-Field相关"><a href="#Receptive-Field相关" class="headerlink" title="Receptive Field相关"></a>Receptive Field相关</h4><p>针对不同的特征，课程中提到的“识别鸟”的任务，可以训练一些特征如鸟喙、鸟腿、翅膀。<br>将输入图像划分成多个Receptive Field，每个Receptive Field由识别多种特征的神经元来检测。<br>相同特征的神经元共享参数，检测不同的Receptive Field（某个特征可能出现在输入图像的不同位置，所以每个Receptive Field都要放检测某一特征的神经元）</p><h4 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h4><p>有时候运算量过大时，可以使用池化减少运算量</p><ul><li>池化挑选一次卷积的结果中的某一个值：最大值（max pooling） 最小值（min pooling） 平均值（mean pooling） </li><li>但是减少运算量的同时会丢失精度</li></ul><h1 id="self-attention"><a href="#self-attention" class="headerlink" title="self attention"></a>self attention</h1><p>$$<br>k &#x3D; W_k * I<br>$$</p><p>$$<br>q &#x3D; W_q * I<br>$$</p><p>$$<br>v &#x3D; W_v * I<br>$$<br>计算第i个输入与第j个输入的相关性<br>$$<br>\alpha_{ij} &#x3D; dot(q_i , k_j )<br>$$<br>得到相关性后，该层的输出<br>$$b_i &#x3D; \sum_i \alpha_{ij} v_i$$<br>将得到的结果通过softmax操作得到一个结果的置信值</p><h2 id="multi-head-self-attention"><a href="#multi-head-self-attention" class="headerlink" title="multi-head self-attention"></a>multi-head self-attention</h2><p>self-attention计算 q、k、v, multi-head将计算$q_1,q_2,k_1,k_2,v_1,v_2$<br>计算相关性：<br>$$\alpha_{ijk} &#x3D; dot(q_{ik} , k_{jk})$$<br>计算输出：<br>$$b_{ik} &#x3D; \sum_i \alpha_{ijk} v_k$$<br>将得到的$b_i1 , b_i2$拼接：<br>$$b_i &#x3D; \begin{bmatrix} W_b\end{bmatrix} \begin{bmatrix} b_{i1}\b_{i2}\end{bmatrix}$$</p><h2 id="positional-encoding"><a href="#positional-encoding" class="headerlink" title="positional encoding"></a>positional encoding</h2><p>给输入向量$I$加入位置信息<br>$$I + e^i$$</p><h1 id="Transformer-encoder-decoder"><a href="#Transformer-encoder-decoder" class="headerlink" title="Transformer : encoder + decoder"></a>Transformer : encoder + decoder</h1><h2 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h2><p>$\begin{bmatrix} x_1 \ x_2 \…\x_n\end{bmatrix}$&#x3D;&gt; block &#x3D;&gt; $\begin{bmatrix} y_1 \ y_2 \…\y_n\end{bmatrix}$  </p><h3 id="block"><a href="#block" class="headerlink" title="block"></a>block</h3><p>$\begin{bmatrix} x_1 \ x_2 \…\x_n\end{bmatrix}$ &#x3D;&gt; self-attention &#x3D;&gt; $\begin{bmatrix} x_1’ \ x_2’ \…\x_n’\end{bmatrix}$  </p><p>$\begin{bmatrix} x_1’ \ x_2’ \…\x_n’\end{bmatrix}$ + $\begin{bmatrix} x_1 \ x_2 \…\x_n\end{bmatrix}$ &#x3D;&gt; norm</p><ul><li>norm :${ x - \mu \over \sigma}$</li></ul><h2 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h2><p>获取encoder的输出作为输入，产生transformer真正的输出<br>$\begin{bmatrix} x_1’ \ x_2’ \…\x_n’\end{bmatrix}$ &#x3D;&gt; decoder<br>token &#x3D;&gt; decoder<br>特殊的输入：给decoder的token中有两个特殊的字符：BOS（begin of sequence）和END<br>masked-self-attention：通常的self-attention每个输出都会看过所有的输入，masked-self-attention只会看之前的输入，不看之后的输入<br>vocabulary-list：transformer输出的token组成的词典</p><h3 id="decoder过程："><a href="#decoder过程：" class="headerlink" title="decoder过程："></a>decoder过程：</h3><ul><li>开始时获取encoder的输出结果，token使用BOS，经过masked-self-attention</li><li>masked-self-attention的输出经过soft-max得到概率分布，选取最大的值对应的vocabulary-list的token作为decoder的输出</li><li>之后的轮次，将decoder每次的输出，累计作为新的输入再次输出，直到输出END为止</li></ul><h3 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h3><p>使用这种方式是顺序串行产生结果，效率较低<br>考虑并行化，NAT（non-autoregressive）：难点在于不确定输出的长度</p><ul><li>1、另外训练一个神经网络，用于产生输出的长度N</li><li>2、给定一个最大值的输出数量，截取到END作为输出</li></ul><h2 id="encoder和decoder的信息传递"><a href="#encoder和decoder的信息传递" class="headerlink" title="encoder和decoder的信息传递"></a>encoder和decoder的信息传递</h2><p>encoder输出$\begin{bmatrix} x_1’ \ x_2’ \…\x_n’\end{bmatrix}$<br>参考self-attention的思想：<br>$$k &#x3D; W_k \begin{bmatrix} x_1’ \ x_2’ \…\x_n’\end{bmatrix}$$<br>$$v &#x3D; W_v \begin{bmatrix} x_1’ \ x_2’ \…\x_n’\end{bmatrix}$$<br>$$q &#x3D; W_q \begin{bmatrix} token’\end{bmatrix}(token’是token经过masked-self-attention得到的向量)$$<br>最终输出$$b &#x3D; \sum_i dot(k_i , q) * v_i$$</p><h2 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h2><ul><li>1、串行输出的decoder在输出一个错误token后将会一步错，步步错<br>ans：在训练时不用decoder的输出作为输入，而是使用ground truth作为输入</li><li>2、有时transformer会忽略输入的一部分或者获得了乱序的输入（语音识别时必须有序才有意义）<br>ans：使用guided-attention，训练时强迫顺序看完训练资料</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> 李宏毅机器学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>about Suchan</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<h1 id="About-Suchan’s-Home"><a href="#About-Suchan’s-Home" class="headerlink" title="About Suchan’s Home"></a>About Suchan’s Home</h1><p>机缘巧合之下，有幸读到《暗时间》这本好书，大受启发，书中提到了许多Suchan平时自己也想到了的观点，但是远远没有作者刘未鹏先生那般凝练、系统，在这本书的强烈建议下，恰逢Suchan也有一些计算机相关的知识和技能，遂于2024年8月3日开始搭建本站。<br>奈何Suchan确实不太擅长前端网页技术，本着内容第一、美观第二的原则，跟着网上的一些教程用hexo连夜起草了这个网站，只能说美化网站的任务，是一个永恒的TODO<br>写出一些精华的文章，结识一些行业内的大牛，想必是每一个博客最初的梦想，Suchan自知在达到“稳定更新”之前什么梦想都是遥不可及的，所以本站的初心只是激励Suchan勤思考、多记录，倘若有幸有一些文章（如果真的更新出来了的话）能被您看到，那将是Suchan不可多得的荣幸。希望来到本站的你，能够与Suchan一同交流进步，共勉~<br>本站目前构想的是编写以下几个板块：</p><ul><li>机器学习相关的学习经验</li><li>游戏引擎的开发</li><li>游戏的开发（主要使用unity）</li><li>一些美术（Maybe）</li><li>一些读书笔记  <blockquote><p>技术栈啊，都学杂了~</p></blockquote></li></ul><h1 id="About-SuchanTso"><a href="#About-SuchanTso" class="headerlink" title="About SuchanTso"></a>About SuchanTso</h1><ul><li>昵称：SuchanTso</li><li>性别： ♂</li><li>出生日期：199x.6.9</li><li>邮箱：<a href="mailto:&#115;&#x75;&#x63;&#x68;&#x61;&#x6e;&#x74;&#115;&#x6f;&#x40;&#x7a;&#106;&#117;&#x2e;&#x65;&#x64;&#117;&#x2e;&#x63;&#110;">&#115;&#x75;&#x63;&#x68;&#x61;&#x6e;&#x74;&#115;&#x6f;&#x40;&#x7a;&#106;&#117;&#x2e;&#x65;&#x64;&#117;&#x2e;&#x63;&#110;</a></li><li>籍贯：云南人（是的家里有大象）</li><li>技术栈：主要还是cpp，最近学习机器学习，恶补python中</li><li>mbti：ENTJ。个人倾向于认为mbti是一个人对自己一段时间的总结，相比其他玄学要更加reality一些</li><li>SuchanTso是什么含义：SuchanTso是我短短二十几年中连续更换的第N个英文名了，其中不乏各种中二的代号，最后进化到了取自名字谐音的Suchan，至于Tso，则是姓氏“左”的英文（又高调又低调捏~</li></ul><h2 id="一些履历"><a href="#一些履历" class="headerlink" title="一些履历"></a>一些履历</h2><p>Suchan手里的是小镇做题家剧本  </p><ul><li>高考进入浙江大学信息工程专业  </li><li>本科毕业两年于小x书当任程序员，主要负责自研游戏引擎的特效开发，所以有一些游戏引擎相关的技术栈。如果您用过该app的“文字”功能，那么很荣幸，Suchan参与开发了其中的大多数样式呢~  </li><li>建站这年考取浙江大学攻读人工智能的研究生。遂加入了AI炼丹大军。</li></ul><p>TO BE CONTINUE…</p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>link</title>
      <link href="/link/index.html"/>
      <url>/link/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
