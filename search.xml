<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>AI-for-beginners_0</title>
      <link href="/2024/08/09/AI-forbeginners-0/"/>
      <url>/2024/08/09/AI-forbeginners-0/</url>
      
        <content type="html"><![CDATA[<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>之前学习李宏毅机器学习的视频课，感觉无穷无尽的视频很容易分散注意力，加之配课程要求的环境有一些阻力，演化成了只看不练的假把式，深思熟虑后认为李宏毅老师的课程不太适合我目前的学习状态。课程讲得固然清晰，但是奈何没法长时间集中注意学习。痛定思痛，找到了微软的AI-for-beginners课程，决定跟着每一讲快速过一遍知识点，把每一个lab做下去。<br>note：截止这一篇文章撰写的时候Suchan做了两讲的内容，发现：</p><ul><li>李宏毅老师的课程将挺多知识点都讲解的十分透彻，自己在看课程的时候遇到的知识点都很快理解并往下做</li><li>微软共有三个AI的课程，分别是<a href="https://github.com/microsoft/ML-For-Beginners">MachineLearning-for-beginners</a>、<a href="https://github.com/microsoft/AI-For-Beginners">AI-for-beginners</a>以及<a href="https://github.com/microsoft/generative-ai-for-beginners">generative-ai-for-beginners</a>，遗憾的是，Suchan选中的AI-for-beginners没有中文教程，只能硬着头皮啃英文版</li><li>再次遗憾的是，Suchan找了挺多网站也没有找到AI-for-beginners里面Lab的solution，只能自己摸索（有找到的小伙伴麻烦评论分享一下呢）</li></ul><h2 id="ok废话少说，启动！"><a href="#ok废话少说，启动！" class="headerlink" title="ok废话少说，启动！"></a>ok废话少说，启动！</h2><h1 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h1><p>教程中提供了三种运行代码的方式，本地构建、云上运行以及云上GPU运行。盲猜后两种云不会是免费的午餐，果断选择了本地方式。</p><h2 id="安装conda"><a href="#安装conda" class="headerlink" title="安装conda"></a>安装conda</h2><p>登录anaconda的<a href="https://www.anaconda.com/download">官网</a>下载，anaconda是一个集成的工具软件。<br>直接跳过登录下载即可<br><img src="/img/AI-for-beginners/conda.png" alt="conda" title="conda"><br>找到自己的平台一路下载安装<br><img src="/img/AI-for-beginners/condaDownload.png" alt="condaDownload" title="condaDownload"><br>直到任务栏中可用<br><img src="/img/AI-for-beginners/condaDownloadSuccess.png" alt="condaDownloadSuccess" title="condaDownloadSuccess"><br>随后跟着课程中的命令，最好把别人的仓库fork一份再克隆自己的，这样自己写的lab提交到自己的仓库可以随时方便查看，会更加清晰一些</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> http://github.com/microsoft/ai-for-beginners</span><br><span class="line"><span class="built_in">cd</span> ai-for-beginners</span><br><span class="line">conda <span class="built_in">env</span> create --name ai4beg --file .devcontainer/environment.yml</span><br><span class="line">conda activate ai4beg</span><br></pre></td></tr></table></figure><h2 id="编辑器"><a href="#编辑器" class="headerlink" title="编辑器"></a>编辑器</h2><p>教程中推荐使用vscode + python Extension来运行，作者发现直接使用pycharm会方便一些（jet brain yyds！！），省去了不少配置的麻烦，一键下载jupyter notebook，唯一的缺点就是需要一些银子。当然也可以遵从教程的做法，毕竟vscode是免费使用的  </p><blockquote><p>此处就不展示如何下载pycharm以及配置vscode了，相信你可以的  </p></blockquote><h3 id="检查环境是否成功"><a href="#检查环境是否成功" class="headerlink" title="检查环境是否成功"></a>检查环境是否成功</h3><p>让我们不妨直接跳到教程的第三节:3-NeuralNetworks&#x2F;03-Perceptron&#x2F;Perceptron.ipynb(前两节大多在讲解AI的发展史以及一两个无关紧要的算法引导。感觉不看也无伤大雅)随便找到一段代码，例如下图引入一些库的代码片段：<br><img src="/img/AI-for-beginners/environmentCheck.png" alt="environmentCheck" title="environmentCheck"><br>点击运行后看到执行成功就是环境配好了，如果报错一些库引入失败，可以尝试在终端中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install xxx</span><br></pre></td></tr></table></figure><p>通常xxx就是你引入的库的名称，如果没法install，尝试把报错复制搜索，能找到具体的install的名称</p><h1 id="What’s-More"><a href="#What’s-More" class="headerlink" title="What’s More"></a>What’s More</h1><p>至此环境搭好，可以开始AI之旅<br>每一节都会有6道题的<a href="https://red-field-0a6ddfd03.1.azurestaticapps.net/">quiz</a>，pre &amp; post Lecture，大多是一些基本概念</p>]]></content>
      
      
      
        <tags>
            
            <tag> AI_for_beginner_microsoft_note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Machine_learning_1</title>
      <link href="/2024/08/04/Machine-learning-1/"/>
      <url>/2024/08/04/Machine-learning-1/</url>
      
        <content type="html"><![CDATA[<h1 id="李宏毅机器学习笔记（一）"><a href="#李宏毅机器学习笔记（一）" class="headerlink" title="李宏毅机器学习笔记（一）"></a>李宏毅机器学习笔记（一）</h1><span id="more"></span><h1 id="问题种类：regression-、-classification-、-structuredLearning"><a href="#问题种类：regression-、-classification-、-structuredLearning" class="headerlink" title="问题种类：regression 、 classification 、 structuredLearning"></a>问题种类：regression 、 classification 、 structuredLearning</h1><p>$$<br>y &#x3D; w * x + b (linear)<br>$$</p><ul><li>训练合适的w和b</li></ul><h2 id="定义loss函数："><a href="#定义loss函数：" class="headerlink" title="定义loss函数："></a>定义loss函数：</h2><ul><li>MAE： regression中预测值与label之间差的绝对值和</li><li>MSE： regression中预测值与label之间差的平方和</li><li>Cross-Entropy：classification中预测值与label的交叉熵</li></ul><h2 id="training：找到loss最小的feature"><a href="#training：找到loss最小的feature" class="headerlink" title="training：找到loss最小的feature"></a>training：找到loss最小的feature</h2><h2 id="optimize"><a href="#optimize" class="headerlink" title="optimize"></a>optimize</h2><ul><li>通常使用梯度下降法：对各个feature求偏导得到梯度，向梯度减小的方向修改参数</li><li>tips：对于斜率变化小的case，在梯度下降的过程中使用相同的步长将很难达到最优的loss。通常使用adam(pytorch自带)、冲量梯度下降法来优化</li></ul><h1 id="DNN"><a href="#DNN" class="headerlink" title="DNN"></a>DNN</h1><p>上述linear的方程没法预测更多一般的情况，通常采用多个ReLU函数或者Sigmoid函数来逼近真实的分布</p><h3 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h3><p>$$y &#x3D; b + \sum_i c_i sigmoid(y_j)$$<br>上式$y_j$也可以写成$y_j &#x3D; b + \sum_j w_jx_j $<br>有$$y &#x3D; b + \sum_i c_i sigmoid(b_i + \sum_j w_{ij}x_i)$$<br>写成矩阵形式<br>$$<br> y &#x3D; b + \begin{bmatrix} c\end{bmatrix}^T + sigmoid(b + \begin{bmatrix} W\end{bmatrix}\begin{bmatrix} x\end{bmatrix})<br>$$</p><ul><li>上式的$y$当作下一层的输入，又可以套一层神经网路，即是简单的DNN模型</li></ul><h3 id="批处理"><a href="#批处理" class="headerlink" title="批处理"></a>批处理</h3><p>训练时候将数据分成多个batch，每个batch计算出一个loss，根据loss来计算梯度，更新feature。</p><ul><li>全部batch计算完称为一个epoch</li></ul><h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><p>CNN通常用于图像操作，输入是一幅图像</p><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p>对于输入图像看作一个矩阵$\begin{bmatrix} Image\end{bmatrix}$ , 定义一个卷积核矩阵$\begin{bmatrix} Filter\end{bmatrix}$与其上的数字相乘相加得到一个值称为一次卷积<br>通过训练出一系列的卷积核一层一层的对图像进行卷积运算提取相关的特征</p><h3 id="一些问题及解决"><a href="#一些问题及解决" class="headerlink" title="一些问题及解决"></a>一些问题及解决</h3><ul><li>卷积没法超出边界导致卷积结果大小 &lt; 原图大小 &#x3D;&gt; 通过给图像边界补0（padding）</li><li>每次卷积都对整个图像操作，计算量大，参数多，容易overfitting &#x3D;&gt; 设定Receptive Field</li></ul><h4 id="Receptive-Field相关"><a href="#Receptive-Field相关" class="headerlink" title="Receptive Field相关"></a>Receptive Field相关</h4><p>针对不同的特征，课程中提到的“识别鸟”的任务，可以训练一些特征如鸟喙、鸟腿、翅膀。<br>将输入图像划分成多个Receptive Field，每个Receptive Field由识别多种特征的神经元来检测。<br>相同特征的神经元共享参数，检测不同的Receptive Field（某个特征可能出现在输入图像的不同位置，所以每个Receptive Field都要放检测某一特征的神经元）</p><h4 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h4><p>有时候运算量过大时，可以使用池化减少运算量</p><ul><li>池化挑选一次卷积的结果中的某一个值：最大值（max pooling） 最小值（min pooling） 平均值（mean pooling） </li><li>但是减少运算量的同时会丢失精度</li></ul><h1 id="self-attention"><a href="#self-attention" class="headerlink" title="self attention"></a>self attention</h1><p>$$<br>k &#x3D; W_k * I<br>$$</p><p>$$<br>q &#x3D; W_q * I<br>$$</p><p>$$<br>v &#x3D; W_v * I<br>$$<br>计算第i个输入与第j个输入的相关性<br>$$<br>\alpha_{ij} &#x3D; dot(q_i , k_j )<br>$$<br>得到相关性后，该层的输出<br>$$b_i &#x3D; \sum_i \alpha_{ij} v_i$$<br>将得到的结果通过softmax操作得到一个结果的置信值</p><h2 id="multi-head-self-attention"><a href="#multi-head-self-attention" class="headerlink" title="multi-head self-attention"></a>multi-head self-attention</h2><p>self-attention计算 q、k、v, multi-head将计算$q_1,q_2,k_1,k_2,v_1,v_2$<br>计算相关性：<br>$$\alpha_{ijk} &#x3D; dot(q_{ik} , k_{jk})$$<br>计算输出：<br>$$b_{ik} &#x3D; \sum_i \alpha_{ijk} v_k$$<br>将得到的$b_i1 , b_i2$拼接：<br>$$b_i &#x3D; \begin{bmatrix} W_b\end{bmatrix} \begin{bmatrix} b_{i1}\b_{i2}\end{bmatrix}$$</p><h2 id="positional-encoding"><a href="#positional-encoding" class="headerlink" title="positional encoding"></a>positional encoding</h2><p>给输入向量$I$加入位置信息<br>$$I + e^i$$</p><h1 id="Transformer-encoder-decoder"><a href="#Transformer-encoder-decoder" class="headerlink" title="Transformer : encoder + decoder"></a>Transformer : encoder + decoder</h1><h2 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h2><p>$\begin{bmatrix} x_1 \ x_2 \…\x_n\end{bmatrix}$&#x3D;&gt; block &#x3D;&gt; $\begin{bmatrix} y_1 \ y_2 \…\y_n\end{bmatrix}$  </p><h3 id="block"><a href="#block" class="headerlink" title="block"></a>block</h3><p>$\begin{bmatrix} x_1 \ x_2 \…\x_n\end{bmatrix}$ &#x3D;&gt; self-attention &#x3D;&gt; $\begin{bmatrix} x_1’ \ x_2’ \…\x_n’\end{bmatrix}$  </p><p>$\begin{bmatrix} x_1’ \ x_2’ \…\x_n’\end{bmatrix}$ + $\begin{bmatrix} x_1 \ x_2 \…\x_n\end{bmatrix}$ &#x3D;&gt; norm</p><ul><li>norm :${ x - \mu \over \sigma}$</li></ul><h2 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h2><p>获取encoder的输出作为输入，产生transformer真正的输出<br>$\begin{bmatrix} x_1’ \ x_2’ \…\x_n’\end{bmatrix}$ &#x3D;&gt; decoder<br>token &#x3D;&gt; decoder<br>特殊的输入：给decoder的token中有两个特殊的字符：BOS（begin of sequence）和END<br>masked-self-attention：通常的self-attention每个输出都会看过所有的输入，masked-self-attention只会看之前的输入，不看之后的输入<br>vocabulary-list：transformer输出的token组成的词典</p><h3 id="decoder过程："><a href="#decoder过程：" class="headerlink" title="decoder过程："></a>decoder过程：</h3><ul><li>开始时获取encoder的输出结果，token使用BOS，经过masked-self-attention</li><li>masked-self-attention的输出经过soft-max得到概率分布，选取最大的值对应的vocabulary-list的token作为decoder的输出</li><li>之后的轮次，将decoder每次的输出，累计作为新的输入再次输出，直到输出END为止</li></ul><h3 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h3><p>使用这种方式是顺序串行产生结果，效率较低<br>考虑并行化，NAT（non-autoregressive）：难点在于不确定输出的长度</p><ul><li>1、另外训练一个神经网络，用于产生输出的长度N</li><li>2、给定一个最大值的输出数量，截取到END作为输出</li></ul><h2 id="encoder和decoder的信息传递"><a href="#encoder和decoder的信息传递" class="headerlink" title="encoder和decoder的信息传递"></a>encoder和decoder的信息传递</h2><p>encoder输出$\begin{bmatrix} x_1’ \ x_2’ \…\x_n’\end{bmatrix}$<br>参考self-attention的思想：<br>$$k &#x3D; W_k \begin{bmatrix} x_1’ \ x_2’ \…\x_n’\end{bmatrix}$$<br>$$v &#x3D; W_v \begin{bmatrix} x_1’ \ x_2’ \…\x_n’\end{bmatrix}$$<br>$$q &#x3D; W_q \begin{bmatrix} token’\end{bmatrix}(token’是token经过masked-self-attention得到的向量)$$<br>最终输出$$b &#x3D; \sum_i dot(k_i , q) * v_i$$</p><h2 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h2><ul><li>1、串行输出的decoder在输出一个错误token后将会一步错，步步错<br>ans：在训练时不用decoder的输出作为输入，而是使用ground truth作为输入</li><li>2、有时transformer会忽略输入的一部分或者获得了乱序的输入（语音识别时必须有序才有意义）<br>ans：使用guided-attention，训练时强迫顺序看完训练资料</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> 李宏毅机器学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>archives</title>
      <link href="/archives/index.html"/>
      <url>/archives/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>about Suchan</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<h1 id="About-Suchan’s-Home"><a href="#About-Suchan’s-Home" class="headerlink" title="About Suchan’s Home"></a>About Suchan’s Home</h1><p>机缘巧合之下，有幸读到《暗时间》这本好书，大受启发，书中提到了许多Suchan平时自己也想到了的观点，但是远远没有作者刘未鹏先生那般凝练、系统，在这本书的强烈建议下，恰逢Suchan也有一些计算机相关的知识和技能，遂于2024年8月3日开始搭建本站。<br>奈何Suchan确实不太擅长前端网页技术，本着内容第一、美观第二的原则，跟着网上的一些教程用hexo连夜起草了这个网站，只能说美化网站的任务，是一个永恒的TODO<br>写出一些精华的文章，结识一些行业内的大牛，想必是每一个博客最初的梦想，Suchan自知在达到“稳定更新”之前什么梦想都是遥不可及的，所以本站的初心只是激励Suchan勤思考、多记录，倘若有幸有一些文章（如果真的更新出来了的话）能被您看到，那将是Suchan不可多得的荣幸。希望来到本站的你，能够与Suchan一同交流进步，共勉~<br>本站目前构想的是编写以下几个板块：</p><ul><li>机器学习相关的学习经验</li><li>游戏引擎的开发</li><li>游戏的开发（主要使用unity）</li><li>一些美术（Maybe）</li><li>一些读书笔记  <blockquote><p>技术栈啊，都学杂了~</p></blockquote></li></ul><h1 id="About-SuchanTso"><a href="#About-SuchanTso" class="headerlink" title="About SuchanTso"></a>About SuchanTso</h1><ul><li>昵称：SuchanTso</li><li>性别： ♂</li><li>出生日期：199x.6.9</li><li>邮箱：<a href="mailto:&#115;&#117;&#x63;&#104;&#97;&#x6e;&#x74;&#115;&#x6f;&#64;&#x7a;&#106;&#x75;&#x2e;&#x65;&#100;&#117;&#x2e;&#99;&#x6e;">&#115;&#117;&#x63;&#104;&#97;&#x6e;&#x74;&#115;&#x6f;&#64;&#x7a;&#106;&#x75;&#x2e;&#x65;&#100;&#117;&#x2e;&#99;&#x6e;</a></li><li>籍贯：云南人（是的家里有大象）</li><li>技术栈：主要还是cpp，最近学习机器学习，恶补python中</li><li>mbti：ENTJ。个人倾向于认为mbti是一个人对自己一段时间的总结，相比其他玄学要更加reality一些</li><li>SuchanTso是什么含义：SuchanTso是我短短二十几年中连续更换的第N个英文名了，其中不乏各种中二的代号，最后进化到了取自名字谐音的Suchan，至于Tso，则是姓氏“左”的英文（又高调又低调捏~</li></ul><h2 id="一些履历"><a href="#一些履历" class="headerlink" title="一些履历"></a>一些履历</h2><p>Suchan手里的是小镇做题家剧本  </p><ul><li>高考进入浙江大学信息工程专业  </li><li>本科毕业两年于小x书当任程序员，主要负责自研游戏引擎的特效开发，所以有一些游戏引擎相关的技术栈。如果您用过该app的“文字”功能，那么很荣幸，Suchan参与开发了其中的大多数样式呢~  </li><li>建站这年考取浙江大学攻读人工智能的研究生。遂加入了AI炼丹大军。</li></ul><p>TO BE CONTINUE…</p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>link</title>
      <link href="/link/index.html"/>
      <url>/link/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
